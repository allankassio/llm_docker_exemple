# ğŸ§  LLM Docker Compose Project

Este projeto demonstra como executar uma aplicaÃ§Ã£o Python que consome um modelo de linguagem (LLM) via API, utilizando dois containers Docker orquestrados com Docker Compose.

---

## ğŸ“ Estrutura do Projeto

```
llm_project/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ model/
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ main.py
```

---

## ğŸš€ Como Rodar

### 1. Requisitos

- Docker
- Docker Compose

### 2. Build e Up

Execute o seguinte comando na raiz do projeto:

```bash
docker compose up --build
```

> Isso iniciarÃ¡ dois containers:
> - `llm-model`: expÃµe uma API de geraÃ§Ã£o de texto usando `transformers` (modelo GPT-2).
> - `python-app`: envia um prompt para o modelo e imprime a resposta.

### 3. Exemplo de SaÃ­da

```
Resposta gerada pelo modelo:
Era uma vez um mundo onde as mÃ¡quinas aprendiam com os humanos e criavam histÃ³rias incrÃ­veis...
```

---

## ğŸ§  Sobre os Containers

### ğŸ”¹ `llm` (modelo)

- Imagem baseada em `python:3.10-slim`
- Usa Hugging Face Transformers e FastAPI
- Exposta na porta `8000`

### ğŸ”¹ `app` (cliente)

- Projeto Python simples com `requests`
- Faz POST para `http://llm:8000/generate`

---

## ğŸ”§ CustomizaÃ§Ã£o

VocÃª pode substituir o modelo GPT-2 por outro, como:

```python
generator = pipeline("text-generation", model="meta-llama/Llama-2-7b-hf")
```

> AtenÃ§Ã£o: modelos maiores exigem GPU e dependÃªncias especÃ­ficas.

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© livre para uso educacional e prototipaÃ§Ã£o. Para modelos com licenÃ§as restritas (como LLaMA), verifique os termos oficiais.

---

## ğŸ¤ ContribuiÃ§Ãµes

SugestÃµes, melhorias e PRs sÃ£o bem-vindos!
